## Conclusions and Learnings
While the MinMax algorithm is powerful in theory, its assumption of a rational opponent can lead to suboptimal performance against irrational players. The reason MiniMax can outperform A* agents and Q-learning agents is that it reasonably evaluates the value of each state and successfully anticipates the opponent's actions. By incorporating adaptive evaluation functions, the algorithm can be improved to handle the unpredictability of opponents more effectively.


Applying the A* algorithm to create an agent in the Splendor game demonstrates that the algorithm can help agents make better decisions in complex strategy games. The design of the heuristic function has a very important impact on the performance of the A* algorithm. By optimizing heuristics, agent performance can be significantly improved. At the same time, some problems with the A* algorithm have also emerged. The A* algorithm has higher computational requirements, which is particularly significant when dealing with complex game states. In Splendor game, the agent needs to make a decision within 1 second. This requires the algorithm to calculate the results in a very short time. Therefore, future exploration should continue to study how to reduce computational costs while ensuring that the performance of the algorithm is not reduced.


Choosing DQN as a game strategy should theoretically give better results, but in practice the performance is far worse than MiniMax and A*. The reason for this is firstly because the rules of splendor are more complex and it is difficult to learn methods that have excellent performance. Secondly, the neural networks are a black box. It is impossible to see from them what strategies the model has actually learned, and it is difficult to debug them directly by visual observation. On top of that, there is too much variability in the model each time it is trained, and even networks trained at the same number of super-misery perform differently by chance. Finally, due to the considerable code implementation, the possibility of coding errors cannot be ruled out. Overall, the accumulation of these factors resulted in a poor DQN.Looking back on this attempt, a complete set of processes should have been established from the beginning to save and accumulate the model parameters and experimental results, which need to be continuously adjusted in experience to achieve the most. In addition, the introduction of various methods should be gradually increased, and should not pursue to add all the methods into the experiment at one time, which is likely to be misled by the coupling relationship between the methods, and thus the optimization scheme is executed.